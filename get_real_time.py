import asyncio
from playwright.async_api import async_playwright
import os
import csv
import gspread
import re

# --- THÃ”NG TIN ÄÄ‚NG NHáº¬P ---
LOGIN_EMAIL = os.environ.get('LOGIN_EMAIL')
LOGIN_PASSWORD = os.environ.get('LOGIN_PASSWORD')
# ------------------------------

# --- THÃ”NG TIN GOOGLE SHEET ---
GOOGLE_SHEET_NAME = "https://docs.google.com/spreadsheets/d/1rCGTw4GdGlR4K-H7hDk8TjjnGh1jL3NgNZLRQ_h8jY8/edit?usp=sharing"
CREDENTIALS_FILE = "credentials.json"
# ------------------------------
async def scrape_novel_top(page):    
    novels = []
    for i in range(2, 22):
        img_selector = f'#app > div:nth-child(2) > main > div > div.md\\:col-span-2.space-y-8 > div:nth-child(4) > div.grid.grid-cols-1.gap-y-6 > div:nth-child({i}) > div.flex-shrink-0 > a > img'
        title_selector = f'#app > div:nth-child(2) > main > div > div.md\\:col-span-2.space-y-8 > div:nth-child(4) > div.grid.grid-cols-1.gap-y-6 > div:nth-child({i}) > div.flex-grow.space-y-2 > div.flex.items-center.space-x-2 > a'
        author_selector = f'#app > div:nth-child(2) > main > div > div.md\\:col-span-2.space-y-8 > div:nth-child(4) > div.grid.grid-cols-1.gap-y-6 > div:nth-child({i}) > div.flex-grow.space-y-2 > div.flex.justify-between.space-x-2.pt-1 > div > span'
        max_selector = f'#app > div:nth-child(2) > main > div > div.md\\:col-span-2.space-y-8 > div:nth-child(4) > div.grid.grid-cols-1.gap-y-6 > div:nth-child({i}) > div.flex-grow.space-y-2 > div.flex.justify-between.space-x-2.pt-1 > span'

        try:
            # 1. áº¢nh: Náº±m trong div.flex-shrink-0
            img = await page.locator(img_selector).get_attribute('src')
            
            # 2. TiÃªu Ä‘á»: Tháº» a trong div.flex.items-center.space-x-2
            title = await page.locator(title_selector).inner_text()

            url = await page.locator(title_selector).get_attribute('href')
            
            # 3. TÃ¡c giáº£: Tháº» span Ä‘áº§u tiÃªn trong cá»¥m div cuá»‘i
            author = await page.locator(author_selector).inner_text()
            
            # 4. Sá»‘ chÆ°Æ¡ng: Tháº» span cuá»‘i cÃ¹ng (thÆ°á»ng lÃ  sibling cá»§a div chá»©a author)
            max_chapter_text = await page.locator(max_selector).inner_text()

            novels.append([
                url.strip().split("/")[-1],
                title.strip(),
                author.strip(),
                '',                
                img.strip(),
                max_chapter_text.strip(),
                ''
            ])
        except Exception as e:
            print(f"âŒ ÄÃ£ xáº£y ra lá»—i nghiÃªm trá»ng: {e}")
            continue
    return novels

async def scrape_novel_list(page):    
    novels = []
    for i in range(2, 102):
        img_selector = f'#app > div:nth-child(2) > main > div > div > div:nth-child(4) > div.grid.grid-cols-1.gap-y-6 > div:nth-child({i}) > div.flex-shrink-0 > a > img'
        title_selector = f'#app > div:nth-child(2) > main > div > div > div:nth-child(4) > div.grid.grid-cols-1.gap-y-6 > div:nth-child({i}) > div.flex-grow.space-y-2 > div.flex.items-center.justify-between.space-x-2 > div.flex.items-center.space-x-2 > a'
        author_selector = f'#app > div:nth-child(2) > main > div > div > div:nth-child(4) > div.grid.grid-cols-1.gap-y-6 > div:nth-child({i}) > div.flex-grow.space-y-2 > div.flex.justify-between.space-x-2.pt-1 > div > span'
        max_selector = f'#app > div:nth-child(2) > main > div > div > div:nth-child(4) > div.grid.grid-cols-1.gap-y-6 > div:nth-child({i}) > div.flex-grow.space-y-2 > div.flex.justify-between.space-x-2.pt-1 > span'

        try:
            # 1. áº¢nh: Náº±m trong div.flex-shrink-0
            img = await page.locator(img_selector).get_attribute('src')
            
            # 2. TiÃªu Ä‘á»: Tháº» a trong div.flex.items-center.space-x-2
            title = await page.locator(title_selector).inner_text()

            url = await page.locator(title_selector).get_attribute('href')
            
            # 3. TÃ¡c giáº£: Tháº» span Ä‘áº§u tiÃªn trong cá»¥m div cuá»‘i
            author = await page.locator(author_selector).inner_text()
            
            # 4. Sá»‘ chÆ°Æ¡ng: Tháº» span cuá»‘i cÃ¹ng (thÆ°á»ng lÃ  sibling cá»§a div chá»©a author)
            max_chapter_text = await page.locator(max_selector).inner_text()

            novels.append([
                url.strip().split("/")[-1],
                title.strip(),
                author.strip(),
                '',                
                img.strip(),
                max_chapter_text.strip(),
                ''
            ])
        except Exception as e:
            print(f"âŒ ÄÃ£ xáº£y ra lá»—i nghiÃªm trá»ng: {e}")
            continue
    return novels

async def scrape_novel_detail(page):
    id_selector = '#app > div:nth-child(2) > div > main > div.space-y-5 > div.block.md\\:flex > div.mb-4.mx-auto.text-center.md\\:mx-0.md\\:text-left > div.space-x-4.mb-6.md\\:mb-8 > div'
    id_selector2 = '#app > div:nth-child(2) > main > div.space-y-5 > div.block.md\\:flex > div.mb-4.mx-auto.text-center.md\\:mx-0.md\\:text-left > div.space-x-4.mb-6.md\\:mb-8 > div'
    
    # Khá»Ÿi táº¡o táº¥t cáº£ cÃ¡c biáº¿n vá»›i giÃ¡ trá»‹ máº·c Ä‘á»‹nh lÃ  chuá»—i rá»—ng
    target_id = ""

    try:        
        # --- Láº¥y ID (data-x-data) ---
        try:
            data_x_data = await page.locator(id_selector2).get_attribute("data-x-data")
            if data_x_data:
                match = re.search(r'\(([^)]+)\)', data_x_data)
                if match:
                    target_id = match.group(1).strip()
                else:
                    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y ID trong thuá»™c tÃ­nh data-x-data.")
            else:
                print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y thuá»™c tÃ­nh data-x-data.")
        except:
            try:
                data_x_data = await page.locator(id_selector).get_attribute("data-x-data")
                if data_x_data:
                    match = re.search(r'\(([^)]+)\)', data_x_data)
                    if match:
                        target_id = match.group(1).strip()
                    else:
                        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y ID trong thuá»™c tÃ­nh data-x-data.")
                else:
                    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y thuá»™c tÃ­nh data-x-data.")
            except Exception as e:
                print(f"âš ï¸ Lá»—i khi láº¥y ID (data-x-data): {e}")
        
        # Tráº£ vá» dictionary, .strip() bÃ¢y giá» Ä‘Ã£ an toÃ n vÃ¬ táº¥t cáº£ Ä‘á»u lÃ  chuá»—i
        return {
            "id": target_id.strip()
        }

    except Exception as e:
        # Khá»‘i except bÃªn ngoÃ i nÃ y sáº½ báº¯t cÃ¡c lá»—i tháº£m há»a (vÃ­ dá»¥: page bá»‹ Ä‘Ã³ng)
        print(f"âŒ Lá»—i nghiÃªm trá»ng khi láº¥y chi tiáº¿t truyá»‡n: {e}")
        return None

async def main():
    """
    HÃ m chÃ­nh Ä‘iá»u khiá»ƒn toÃ n bá»™ quÃ¡ trÃ¬nh: Ä‘Äƒng nháº­p, duyá»‡t vÃ  lÆ°u chÆ°Æ¡ng.
    """
    # Láº¥y thÃ´ng tin proxy tá»« biáº¿n mÃ´i trÆ°á»ng
    PROXY_SERVER = os.environ.get('PROXY_SERVER')
    PROXY_USER = os.environ.get('PROXY_USER')
    PROXY_PASS = os.environ.get('PROXY_PASS')

    proxy_settings = None
    if PROXY_SERVER:
        proxy_settings = {
            "server": f"http://{PROXY_SERVER}",
            "username": PROXY_USER,
            "password": PROXY_PASS
        }
        print(f"--- Äang sá»­ dá»¥ng proxy: {PROXY_SERVER} ---")
    else:
        print("--- KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin proxy, cháº¡y khÃ´ng qua proxy ---")

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy=proxy_settings
        )
        context = await browser.new_context()
        page = await context.new_page()
        page.set_default_timeout(10000) # 60 giÃ¢y
        try:
            # --- PHáº¦N 1: ÄÄ‚NG NHáº¬P (Chá»‰ cháº¡y má»™t láº§n) ---
            print("Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh Ä‘Äƒng nháº­p...")
            await page.goto("https://metruyencv.com", wait_until="domcontentloaded")
            
            menu_icon_locator = page.locator('svg:has(path[d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"])')
            await menu_icon_locator.wait_for(state="visible")
            await menu_icon_locator.click()
            await page.get_by_role("button", name="ÄÄƒng nháº­p").click()
            await page.get_by_placeholder("email").fill(LOGIN_EMAIL)
            await page.get_by_placeholder("password").fill(LOGIN_PASSWORD)
            await page.get_by_role("button", name="ÄÄƒng nháº­p").click()
            print("ÄÄƒng nháº­p thÃ nh cÃ´ng!")
            # --- PHáº¦N 0: Káº¾T Ná»I VÃ€ KIá»‚M TRA GOOGLE SHEET ---
            print("Äang káº¿t ná»‘i tá»›i Google Sheets...")
            gc = gspread.service_account(filename=CREDENTIALS_FILE)
            sh = gc.open_by_url(GOOGLE_SHEET_NAME)
            print("Káº¿t ná»‘i thÃ nh cÃ´ng!")

            # Má»Ÿ hoáº·c táº¡o sheet Database_ID Ä‘á»ƒ tra cá»©u
            try:
                ws_db = sh.worksheet("Database_ID")
            except gspread.WorksheetNotFound:
                ws_db = sh.add_worksheet(title="Database_ID", rows="1000", cols="2")
                ws_db.append_row(['Url', 'ID'])

            # 2. Táº£i dá»¯ liá»‡u ID Ä‘Ã£ cÃ³ vÃ o Dictionary Ä‘á»ƒ tra cá»©u cá»±c nhanh
            existing_data = ws_db.get_all_values()[1:] # Bá» header
            id_map = {row[0]: row[1] for row in existing_data if len(row) >= 2}

            # --- Xá»¬ LÃ TRANG THá»œI GIAN THá»°C ---
            print("Äang láº¥y danh sÃ¡ch truyá»‡n thá»i gian thá»±c...")
            await page.goto("https://metruyencv.com/thoi-gian-thuc", wait_until="domcontentloaded")

            # Láº¥y dá»¯ liá»‡u tá»« hÃ m Ä‘Ã£ viáº¿t
            novel_list_data = await scrape_novel_list(page)

            new_ids_to_save = []

            # 4. Duyá»‡t vÃ  kiá»ƒm tra
            for novel in novel_list_data:
                url = novel[0]
                
                if url in id_map:
                    # Náº¿u Ä‘Ã£ cÃ³ ID rá»“i thÃ¬ láº¥y luÃ´n, khÃ´ng cáº§n truy cáº­p link
                    novel.append(id_map[url])
                    print(f"â© ÄÃ£ cÃ³ ID cho: {novel[1]} (Bá» qua)")
                else:
                    # Náº¿u chÆ°a cÃ³ má»›i truy cáº­p
                    print(f"ğŸ” Äang cÃ o ID má»›i cho: {novel[1]}")
                    try:
                        response = await page.goto('https://metruyencv.com/truyen/'+url, wait_until="domcontentloaded")
                        if response and response.status == 404:
                            continue
                        scraped_data = await scrape_novel_detail(page)
                        if scraped_data and scraped_data['id']:
                            target_id = scraped_data['id']
                            novel.append(target_id)
                            # ThÃªm vÃ o danh sÃ¡ch Ä‘á»ƒ cáº­p nháº­t Database
                            new_ids_to_save.append([url, target_id])
                            id_map[url] = target_id # Cáº­p nháº­t map Ä‘á»ƒ trÃ¡nh trÃ¹ng trong cÃ¹ng 1 phiÃªn
                        else:
                            novel.append("")
                    except Exception as e:
                        print(f"âŒ Lá»—i khi cÃ o {url}: {e}")
                        novel.append("")            

            # 6. Ghi káº¿t quáº£ cuá»‘i cÃ¹ng vÃ o sheet list_realtime (Ghi Ä‘Ã¨ ná»™i dung má»›i nháº¥t)
            ws_realtime = sh.worksheet("list_realtime")
            ws_realtime.update(range_name='A2', values=novel_list_data)


            # --- Xá»¬ LÃ TRANG TOP ---
            print("Äang láº¥y danh sÃ¡ch truyá»‡n top...")
            await page.goto("https://metruyencv.com/xep-hang/de-cu", wait_until="domcontentloaded")

            for i in range(5):
                # Láº¥y dá»¯ liá»‡u tá»« hÃ m Ä‘Ã£ viáº¿t
                novel_list_data = await scrape_novel_top(page)            

                for novel in novel_list_data:
                    url = novel[0]
                    if url in id_map:
                        # Náº¿u Ä‘Ã£ cÃ³ ID rá»“i thÃ¬ láº¥y luÃ´n, khÃ´ng cáº§n truy cáº­p link
                        novel.append(id_map[url])
                        print(f"â© ÄÃ£ cÃ³ ID cho: {novel[1]} (Bá» qua)")
                    elif url in new_ids_to_save:
                        novel.append(id_map[url])
                        print(f"â© ÄÃ£ cÃ³ ID cho: {novel[1]} (Bá» qua)")
                    else:
                        # Náº¿u chÆ°a cÃ³ má»›i truy cáº­p
                        print(f"ğŸ” Äang cÃ o ID má»›i cho: {novel[1]}")
                        try:
                            response = await page.goto('https://metruyencv.com/truyen/'+url, wait_until="domcontentloaded")
                            if response and response.status == 404:
                                continue
                            scraped_data = await scrape_novel_detail(page)
                            if scraped_data and scraped_data['id']:
                                target_id = scraped_data['id']
                                novel.append(target_id)
                                # ThÃªm vÃ o danh sÃ¡ch Ä‘á»ƒ cáº­p nháº­t Database
                                new_ids_to_save.append([url, target_id])
                                id_map[url] = target_id # Cáº­p nháº­t map Ä‘á»ƒ trÃ¡nh trÃ¹ng trong cÃ¹ng 1 phiÃªn
                            else:
                                novel.append("")
                        except Exception as e:
                            print(f"âŒ Lá»—i khi cÃ o {url}: {e}")
                            novel.append("")
                
                ws_realtime = sh.worksheet("list_top")
                ws_realtime.update(range_name=f'A{20*i+2}', values=novel_list_data)

                if i < 4:
                    await page.goto("https://metruyencv.com/xep-hang/de-cu", wait_until="domcontentloaded")
                    for _ in range(i+1):
                        await page.locator('svg:has(path[d="M9 6l6 6l-6 6"])').click()
            # 5. Cáº­p nháº­t ID má»›i vÃ o sheet Database_ID (Ghi ná»‘i tiáº¿p vÃ o cuá»‘i)
            if new_ids_to_save:
                ws_db.append_rows(new_ids_to_save)
                print(f"âœ… ÄÃ£ lÆ°u thÃªm {len(new_ids_to_save)} ID má»›i vÃ o Database.")

        except Exception as e:
            print(f"âŒ ÄÃ£ xáº£y ra lá»—i nghiÃªm trá»ng: {e}")
            try:
                await page.screenshot(path='screenshots/00_ERROR.png')
                print("ÄÃ£ chá»¥p áº£nh mÃ n hÃ¬nh lá»—i.")
            except Exception as screenshot_error:
                print(f"KhÃ´ng thá»ƒ chá»¥p áº£nh mÃ n hÃ¬nh: {screenshot_error}")

        finally:
            print("\nQuÃ¡ trÃ¬nh Ä‘Ã£ hoÃ n táº¥t. ÄÃ³ng trÃ¬nh duyá»‡t.")
            await browser.close()

# Cháº¡y script
if __name__ == "__main__":
    asyncio.run(main())
